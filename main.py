import pandas as pd
import re
from typing import List
import chromadb
from sentence_transformers import SentenceTransformer
import os
import shutil

def setup_environment():
    """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏ –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö"""
    print("–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è...")
    
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    os.makedirs("./data/dataset/texts", exist_ok=True)
    os.makedirs("./data/results", exist_ok=True)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤
    required_files = [
        "./data/dataset/questions.csv",
        "./data/dataset/texts.csv",
        "./data/dataset/texts/1.txt",
        "./data/dataset/texts/2.txt",
        "./data/dataset/texts/3.txt",
        "./data/dataset/texts/5.txt"
    ]
    
    missing_files = [f for f in required_files if not os.path.exists(f)]
    if missing_files:
        print(f"–û—à–∏–±–∫–∞: –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç —Ñ–∞–π–ª—ã: {missing_files}")
        print("–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –≤—ã –∑–∞–≥—Ä—É–∑–∏–ª–∏ –≤—Å–µ —Ñ–∞–π–ª—ã –¥–∞—Ç–∞—Å–µ—Ç–∞")
        return False
    
    return True

def load_data():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö"""
    print("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    
    try:
        # –ó–∞–≥—Ä—É–∑–∫–∞ –≤–æ–ø—Ä–æ—Å–æ–≤
        q = pd.read_csv("./data/dataset/questions.csv")
        print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ –≤–æ–ø—Ä–æ—Å–æ–≤: {len(q)}")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        docs = pd.read_csv("./data/dataset/texts.csv")
        print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(docs)}")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        raw_texts = []
        successful_loads = 0
        
        for index, row in docs.iterrows():
            try:
                file_path = f"./data/dataset/texts/{row['page_id']}.txt"
                with open(file_path, "r", encoding='utf-8') as f:
                    text = f.read()
                    raw_texts.append(text)
                    successful_loads += 1
            except FileNotFoundError:
                print(f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –§–∞–π–ª {file_path} –Ω–µ –Ω–∞–π–¥–µ–Ω")
                raw_texts.append("")  # –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç
        
        docs["text"] = raw_texts
        print(f"–£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ —Ç–µ–∫—Å—Ç–æ–≤: {successful_loads}/{len(docs)}")
        
        return q, docs
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö: {e}")
        return None, None

# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–æ—Å—Ç–∞—é—Ç—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)
def split_by_sentences(text: str) -> List[str]:
    """–†–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º"""
    if not text or len(text.strip()) == 0:
        return []
    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\s+', text) if len(s.strip()) > 10]
    return sentences

def split_by_paragraphs(text: str) -> List[str]:
    """–†–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ –∞–±–∑–∞—Ü–∞–º"""
    if not text or len(text.strip()) == 0:
        return []
    paragraphs = [p.strip() for p in text.split("\n\n") if len(p.strip()) > 20]
    return paragraphs

def split_by_fixed_length(text: str, max_words: int = 100) -> List[str]:
    """–†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ –∫—É—Å–∫–∏ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –¥–ª–∏–Ω—ã"""
    if not text or len(text.strip()) == 0:
        return []
    words = text.split()
    chunks = []
    for i in range(0, len(words), max_words):
        chunk = " ".join(words[i:i+max_words])
        chunks.append(chunk)
    return chunks

def split_with_overlap(text: str, chunk_size: int = 100, overlap: int = 20) -> List[str]:
    """–†–∞–∑–±–∏–µ–Ω–∏–µ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º"""
    if not text or len(text.strip()) == 0:
        return []
    words = text.split()
    chunks = []
    start = 0
    while start < len(words):
        end = start + chunk_size
        chunk = " ".join(words[start:end])
        chunks.append(chunk)
        start += chunk_size - overlap
    return chunks

def split_by_headings(text: str) -> List[str]:
    """–†–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º (h1, h2, h3)"""
    if not text or len(text.strip()) == 0:
        return []
    
    # –†–µ–≥—É–ª—è—Ä–Ω–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –ø–æ–∏—Å–∫–∞ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
    heading_pattern = r'(^h\d\.\s+.*?)(?=\n\n|$|\\nh\d\.\s+)'
    chunks = []
    
    # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏
    matches = list(re.finditer(heading_pattern, text, re.DOTALL | re.MULTILINE))
    
    if not matches:
        # –ï—Å–ª–∏ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –Ω–µ—Ç, —Ä–∞–∑–±–∏–≤–∞–µ–º –ø–æ –∞–±–∑–∞—Ü–∞–º
        return split_by_paragraphs(text)
    
    for i, match in enumerate(matches):
        start_pos = match.start()
        
        if i < len(matches) - 1:
            # –¢–µ–∫—Å—Ç –æ—Ç —Ç–µ–∫—É—â–µ–≥–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –¥–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ
            end_pos = matches[i + 1].start()
            chunk_content = text[start_pos:end_pos].strip()
        else:
            # –ü–æ—Å–ª–µ–¥–Ω–∏–π chunk - –¥–æ –∫–æ–Ω—Ü–∞ —Ç–µ–∫—Å—Ç–∞
            chunk_content = text[start_pos:].strip()
        
        if len(chunk_content) > 10:
            chunks.append(chunk_content)
    
    return chunks

def create_chunks_dataset(docs_df, mode="paragraphs"):
    """–°–æ–∑–¥–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ —Ä–∞–∑–±–∏—Ç—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    chunks_data = []
    
    for index, row in docs_df.iterrows():
        text = row["text"]
        page_id = row["page_id"]
        title = row["title"]
        
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ —Ç–µ–∫—Å—Ç—ã
        if not text or len(text.strip()) == 0:
            continue
            
        if mode == "sentences":
            chunks = split_by_sentences(text)
        elif mode == "paragraphs":
            chunks = split_by_paragraphs(text)
        elif mode == "fixed":
            chunks = split_by_fixed_length(text, max_words=150)
        elif mode == "overlap":
            chunks = split_with_overlap(text, chunk_size=150, overlap=30)
        elif mode == "headings":
            chunks = split_by_headings(text)
        else:
            chunks = split_by_paragraphs(text)  # –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        
        for chunk_idx, chunk_text in enumerate(chunks):
            if len(chunk_text.strip()) > 10:  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ chunks
                chunks_data.append({
                    "text": chunk_text,
                    "page_id": page_id,
                    "title": title,
                    "chunk_id": f"{page_id}_{chunk_idx}",
                    "chunk_index": chunk_idx
                })
    
    return pd.DataFrame(chunks_data)

def evaluate_strategy(chunks_df, strategy_name, q, top_k=5):
    """–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–æ–∏—Å–∫–∞ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏"""
    
    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –∫–æ–ª–ª–µ–∫—Ü–∏—é –≤ ChromaDB
    client = chromadb.PersistentClient(f"./data/chroma_{strategy_name}")
    
    # –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—É—é –∫–æ–ª–ª–µ–∫—Ü–∏—é –µ—Å–ª–∏ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    try:
        client.delete_collection(strategy_name)
    except:
        pass
    
    collection = client.get_or_create_collection(
        name=strategy_name,
        metadata={"hnsw:space": "cosine"}
    )
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    print("  –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
    model = SentenceTransformer("ai-forever/ru-en-RoSBERTa")
    
    # –î–æ–±–∞–≤–ª—è–µ–º chunks –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É
    print(f"  –î–æ–±–∞–≤–ª–µ–Ω–∏–µ {len(chunks_df)} chunks –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É...")
    
    # –î–æ–±–∞–≤–ª—è–µ–º –ø–∞—á–∫–∞–º–∏ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
    batch_size = 50
    for i in range(0, len(chunks_df), batch_size):
        batch = chunks_df.iloc[i:i+batch_size]
        
        texts = batch["text"].tolist()
        embeddings = model.encode(texts)
        metadatas = [{"page_id": row["page_id"], "chunk_id": row["chunk_id"]} 
                    for _, row in batch.iterrows()]
        ids = [f"id_{i+j}" for j in range(len(batch))]
        
        collection.add(
            ids=ids,
            documents=texts,
            embeddings=embeddings.tolist(),
            metadatas=metadatas
        )
    
    # –û—Ü–µ–Ω–∫–∞ MRR
    print("  –û—Ü–µ–Ω–∫–∞ MRR...")
    df_data = []
    
    for index, row in q.iterrows():
        question = row["question"]
        target_page_id = row["page_id"]
        query = model.encode(question)
        
        results = collection.query(query_embeddings=[query.tolist()], n_results=top_k)
        
        found = False
        for n, (meta, score) in enumerate(
            zip(results["metadatas"][0], results["distances"][0]),
            start=1
        ):
            retrieved_page_id = meta["page_id"]
            if str(target_page_id) == str(retrieved_page_id):
                data_row = [question, n, score]
                df_data.append(data_row)
                found = True
                break
        
        if not found:
            data_row = [question, 0, 0]
            df_data.append(data_row)
    
    # –†–∞—Å—á–µ—Ç MRR
    df = pd.DataFrame(df_data, columns=["question", "position", "score"])
    df["rank"] = df["position"].apply(lambda x: 1 / x if x != 0 else 0)
    MRR = df["rank"].sum() / len(q)
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    hits_at_1 = len(df[df["position"] == 1])
    hits_at_3 = len(df[df["position"] <= 3])
    hits_at_5 = len(df[df["position"] <= 5])
    
    return {
        "MRR": MRR,
        "Hits@1": hits_at_1 / len(q),
        "Hits@3": hits_at_3 / len(q), 
        "Hits@5": hits_at_5 / len(q),
        "details": df
    }

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("=" * 60)
    print("RAG SYSTEM - –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø –†–ê–ó–ë–ò–ï–ù–ò–Ø –î–û–ö–£–ú–ï–ù–¢–û–í")
    print("=" * 60)
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è
    if not setup_environment():
        return
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    q, docs = load_data()
    if q is None or docs is None:
        print("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã.")
        return
    
    print(f"\n–î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã:")
    print(f"  - –í–æ–ø—Ä–æ—Å–æ–≤: {len(q)}")
    print(f"  - –î–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(docs)}")
    
    # –°–æ–∑–¥–∞–µ–º chunks –¥–ª—è –≤—Å–µ—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π
    strategies = ["paragraphs", "sentences", "fixed", "overlap", "headings"]
    chunks_datasets = {}
    
    print(f"\n–°–æ–∑–¥–∞–Ω–∏–µ chunks –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π...")
    for strategy in strategies:
        print(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏: {strategy}")
        chunks_df = create_chunks_dataset(docs, mode=strategy)
        chunks_datasets[strategy] = chunks_df
        print(f"  - –°–æ–∑–¥–∞–Ω–æ {len(chunks_df)} chunks")
    
    # –û—Ü–µ–Ω–∫–∞ –≤—Å–µ—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π
    print(f"\n–û—Ü–µ–Ω–∫–∞ –≤—Å–µ—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π —Ä–∞–∑–±–∏–µ–Ω–∏—è...")
    results = {}
    
    for strategy in strategies:
        print(f"\n–û—Ü–µ–Ω–∫–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏: {strategy}")
        chunks_df = chunks_datasets[strategy]
        result = evaluate_strategy(chunks_df, strategy, q)
        results[strategy] = result
        print(f"  MRR: {result['MRR']:.3f}")
        print(f"  Hits@1: {result['Hits@1']:.3f}")
        print(f"  Hits@3: {result['Hits@3']:.3f}")
        print(f"  Hits@5: {result['Hits@5']:.3f}")
    
    # –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print("\n" + "=" * 60)
    print("–°–†–ê–í–ù–ò–¢–ï–õ–¨–ù–ê–Ø –¢–ê–ë–õ–ò–¶–ê –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
    print("=" * 60)
    
    comparison_data = []
    for strategy in strategies:
        result = results[strategy]
        comparison_data.append({
            "Strategy": strategy,
            "MRR": f"{result['MRR']:.3f}",
            "Hits@1": f"{result['Hits@1']:.3f}",
            "Hits@3": f"{result['Hits@3']:.3f}",
            "Hits@5": f"{result['Hits@5']:.3f}",
            "Chunks Count": len(chunks_datasets[strategy])
        })
    
    comparison_df = pd.DataFrame(comparison_data)
    print(comparison_df.to_string(index=False))
    
    # –ù–∞—Ö–æ–¥–∏–º –ª—É—á—à—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é
    best_strategy = max(results.keys(), key=lambda x: results[x]['MRR'])
    best_result = results[best_strategy]
    
    print(f"\nüéâ –õ–£–ß–®–ê–Ø –°–¢–†–ê–¢–ï–ì–ò–Ø: {best_strategy}")
    print(f"üìä MRR: {best_result['MRR']:.3f}")
    print(f"üéØ Hits@1: {best_result['Hits@1']:.3f}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    output_dir = "./data/results"
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
    comparison_df.to_csv(f"{output_dir}/strategies_comparison.csv", index=False, encoding='utf-8')
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–µ—Ç–∞–ª–∏ –ª—É—á—à–µ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
    best_details = results[best_strategy]['details']
    best_details.to_csv(f"{output_dir}/best_strategy_{best_strategy}_details.csv", index=False, encoding='utf-8')
    
    print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é: {output_dir}")
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ chunks
    print(f"\nüìà –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û CHUNKS:")
    for strategy in strategies:
        chunks_df = chunks_datasets[strategy]
        if len(chunks_df) > 0:
            avg_length = chunks_df['text'].str.split().str.len().mean()
        else:
            avg_length = 0
        print(f"{strategy:12} | {len(chunks_df):4} chunks | –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞: {avg_length:.1f} —Å–ª–æ–≤")
    
    print(f"\n‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")

if __name__ == "__main__":
    main()