import pandas as pd
import re
import numpy as np
from typing import List, Dict, Tuple
import chromadb
from sentence_transformers import SentenceTransformer
import os
import gc
import time
from dataclasses import dataclass
import json

@dataclass
class ChunkingStrategy:
    name: str
    function: callable
    params: Dict
    param_ranges: Dict

class ChunkOptimizer:
    def __init__(self, docs, questions):
        self.docs = docs
        self.questions = questions
        self.model = None
        self.results = []
        
    def initialize_model(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
        if self.model is None:
            print("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
            self.model = SentenceTransformer("ai-forever/ru-en-RoSBERTa")
    
    def clear_memory(self):
        """–û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏"""
        gc.collect()
    
    def split_sentences(self, text: str, min_sentence_length: int = 10, **kwargs) -> List[str]:
        """–†–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º —Å –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω–æ–π"""
        if not text or len(text.strip()) == 0:
            return []
        sentences = [s.strip() for s in re.split(r'(?<=[.!?])\s+', text) 
                    if len(s.strip()) > min_sentence_length]
        return sentences
    
    def split_paragraphs(self, text: str, min_paragraph_length: int = 30, **kwargs) -> List[str]:
        """–†–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ –∞–±–∑–∞—Ü–∞–º —Å –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω–æ–π"""
        if not text or len(text.strip()) == 0:
            return []
        paragraphs = [p.strip() for p in text.split("\n\n") 
                     if len(p.strip()) > min_paragraph_length]
        return paragraphs
    
    def split_fixed(self, text: str, chunk_size: int = 100, **kwargs) -> List[str]:
        """–†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ –∫—É—Å–∫–∏ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –¥–ª–∏–Ω—ã"""
        if not text or len(text.strip()) == 0:
            return []
        words = text.split()
        chunks = []
        for i in range(0, len(words), chunk_size):
            chunk = " ".join(words[i:i + chunk_size])
            if len(chunk.strip()) > kwargs.get('min_chunk_length', 20):
                chunks.append(chunk)
        return chunks
    
    def split_overlap(self, text: str, chunk_size: int = 100, overlap: int = 20, **kwargs) -> List[str]:
        """–†–∞–∑–±–∏–µ–Ω–∏–µ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º"""
        if not text or len(text.strip()) == 0:
            return []
        words = text.split()
        chunks = []
        start = 0
        while start < len(words):
            end = start + chunk_size
            chunk = " ".join(words[start:end])
            if len(chunk.strip()) > kwargs.get('min_chunk_length', 20):
                chunks.append(chunk)
            start += chunk_size - overlap
        return chunks
    
    def split_headings(self, text: str, min_chunk_length: int = 50, **kwargs) -> List[str]:
        """–†–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º"""
        if not text or len(text.strip()) == 0:
            return []
        
        # –ò—â–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ h1., h2., h3.
        heading_pattern = r'(^h[1-3]\.\s+.*?)(?=\n\n|$|\nh[1-3]\.\s+)'
        chunks = []
        
        matches = list(re.finditer(heading_pattern, text, re.DOTALL | re.MULTILINE))
        
        if not matches:
            return self.split_paragraphs(text, min_paragraph_length=min_chunk_length)
        
        for i, match in enumerate(matches):
            start_pos = match.start()
            
            if i < len(matches) - 1:
                end_pos = matches[i + 1].start()
                chunk_content = text[start_pos:end_pos].strip()
            else:
                chunk_content = text[start_pos:].strip()
            
            if len(chunk_content) > min_chunk_length:
                chunks.append(chunk_content)
        
        return chunks
    
    def create_chunks_dataset(self, strategy_func: callable, strategy_params: Dict) -> pd.DataFrame:
        """–°–æ–∑–¥–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ —Ä–∞–∑–±–∏—Ç—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –∑–∞–¥–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏"""
        chunks_data = []
        
        for index, row in self.docs.iterrows():
            text = row["text"]
            page_id = row["page_id"]
            
            if not text or len(text.strip()) == 0:
                continue
                
            chunks = strategy_func(text, **strategy_params)
            
            for chunk_idx, chunk_text in enumerate(chunks):
                if len(chunk_text.strip()) > strategy_params.get('min_chunk_length', 30):
                    chunks_data.append({
                        "text": chunk_text,
                        "page_id": page_id,
                        "chunk_id": f"{page_id}_{chunk_idx}",
                        "words_count": len(chunk_text.split())
                    })
        
        return pd.DataFrame(chunks_data)
    
    def evaluate_strategy(self, chunks_df: pd.DataFrame, strategy_name: str, params: Dict) -> Dict:
        """–û—Ü–µ–Ω–∫–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Å –∑–∞–¥–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏"""
        self.clear_memory()
        
        # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ –∏–º—è –¥–ª—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏
        collection_name = f"{strategy_name}_{hash(str(params))}"
        
        # –°–æ–∑–¥–∞–µ–º –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É
        client = chromadb.PersistentClient(f"./data/chroma_optimization")
        
        try:
            client.delete_collection(collection_name)
        except:
            pass
        
        collection = client.get_or_create_collection(
            name=collection_name,
            metadata={"hnsw:space": "cosine"}
        )
        
        # –î–æ–±–∞–≤–ª—è–µ–º chunks –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É
        batch_size = 30
        total_chunks = len(chunks_df)
        
        for i in range(0, total_chunks, batch_size):
            batch = chunks_df.iloc[i:i + batch_size]
            
            texts = batch["text"].tolist()
            embeddings = self.model.encode(texts, show_progress_bar=False)
            metadatas = [{"page_id": row["page_id"], "chunk_id": row["chunk_id"]} 
                        for _, row in batch.iterrows()]
            ids = [f"id_{i+j}" for j in range(len(batch))]
            
            collection.add(
                ids=ids,
                documents=texts,
                embeddings=embeddings.tolist(),
                metadatas=metadatas
            )
        
        # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
        df_data = []
        
        for _, row in self.questions.iterrows():
            question = row["question"]
            target_page_id = row["page_id"]
            query = self.model.encode(question)
            
            results = collection.query(
                query_embeddings=[query.tolist()], 
                n_results=5
            )
            
            found = False
            for n, (meta, score) in enumerate(
                zip(results["metadatas"][0], results["distances"][0]),
                start=1
            ):
                retrieved_page_id = meta["page_id"]
                if str(target_page_id) == str(retrieved_page_id):
                    data_row = [question, n, score]
                    df_data.append(data_row)
                    found = True
                    break
            
            if not found:
                data_row = [question, 0, 0]
                df_data.append(data_row)
        
        # –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫
        df = pd.DataFrame(df_data, columns=["question", "position", "score"])
        df["rank"] = df["position"].apply(lambda x: 1 / x if x != 0 else 0)
        MRR = df["rank"].sum() / len(self.questions)
        
        hits_at_1 = len(df[df["position"] == 1])
        hits_at_3 = len(df[df["position"] <= 3])
        hits_at_5 = len(df[df["position"] <= 5])
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ chunks
        avg_words = chunks_df['words_count'].mean() if len(chunks_df) > 0 else 0
        total_chunks = len(chunks_df)
        
        # –û—á–∏—Å—Ç–∫–∞
        try:
            client.delete_collection(collection_name)
        except:
            pass
        
        self.clear_memory()
        
        return {
            "strategy": strategy_name,
            "params": params.copy(),
            "MRR": MRR,
            "Hits@1": hits_at_1 / len(self.questions),
            "Hits@3": hits_at_3 / len(self.questions),
            "Hits@5": hits_at_5 / len(self.questions),
            "avg_chunk_words": avg_words,
            "total_chunks": total_chunks,
            "chunk_efficiency": MRR / max(1, total_chunks / len(self.docs))  # MRR –Ω–∞ chunk
        }
    
    def generate_parameter_combinations(self, strategy_name: str) -> List[Dict]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–º–±–∏–Ω–∞—Ü–∏–π –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏"""
        param_combinations = []
        
        if strategy_name == "sentences":
            for min_length in [5, 10, 15, 20]:
                param_combinations.append({
                    'min_sentence_length': min_length,
                    'min_chunk_length': min_length * 3
                })
                
        elif strategy_name == "paragraphs":
            for min_length in [20, 30, 50, 80]:
                param_combinations.append({
                    'min_paragraph_length': min_length,
                    'min_chunk_length': min_length
                })
                
        elif strategy_name == "fixed":
            for chunk_size in [50, 100, 150, 200, 250]:
                param_combinations.append({
                    'chunk_size': chunk_size,
                    'min_chunk_length': 20
                })
                
        elif strategy_name == "overlap":
            for chunk_size in [100, 150, 200]:
                for overlap in [10, 20, 30, 40]:
                    if overlap < chunk_size:
                        param_combinations.append({
                            'chunk_size': chunk_size,
                            'overlap': overlap,
                            'min_chunk_length': 20
                        })
                        
        elif strategy_name == "headings":
            for min_length in [30, 50, 80, 100]:
                param_combinations.append({
                    'min_chunk_length': min_length
                })
        
        return param_combinations
    
    def optimize_strategy(self, strategy_name: str, max_experiments: int = 10):
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏"""
        print(f"\nüîß –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏: {strategy_name}")
        print("=" * 50)
        
        self.initialize_model()
        
        # –ü–æ–ª—É—á–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
        strategy_func = getattr(self, f"split_{strategy_name}")
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        param_combinations = self.generate_parameter_combinations(strategy_name)
        
        if len(param_combinations) > max_experiments:
            # –í—ã–±–∏—Ä–∞–µ–º –Ω–∞–∏–±–æ–ª–µ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏
            param_combinations = param_combinations[:max_experiments]
        
        strategy_results = []
        
        for i, params in enumerate(param_combinations):
            print(f"–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç {i+1}/{len(param_combinations)}: {params}")
            
            try:
                # –°–æ–∑–¥–∞–µ–º chunks
                chunks_df = self.create_chunks_dataset(strategy_func, params)
                
                if len(chunks_df) == 0:
                    print("  ‚ö†Ô∏è  –ù–µ —Å–æ–∑–¥–∞–Ω–æ chunks, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                    continue
                
                # –û—Ü–µ–Ω–∏–≤–∞–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—é
                result = self.evaluate_strategy(chunks_df, strategy_name, params)
                strategy_results.append(result)
                
                print(f"  ‚úÖ MRR: {result['MRR']:.3f}, Chunks: {result['total_chunks']}, "
                      f"Avg words: {result['avg_chunk_words']:.1f}")
                
            except Exception as e:
                print(f"  ‚ùå –û—à–∏–±–∫–∞: {e}")
                continue
        
        return strategy_results
    
    def run_comprehensive_optimization(self):
        """–ó–∞–ø—É—Å–∫ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤—Å–µ—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π"""
        print("üöÄ –ó–ê–ü–£–°–ö –ö–û–ú–ü–õ–ï–ö–°–ù–û–ô –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ò –ß–ê–ù–ö–û–í")
        print("=" * 60)
        
        strategies = ["sentences", "paragraphs", "fixed", "overlap", "headings"]
        all_results = []
        
        for strategy in strategies:
            strategy_results = self.optimize_strategy(strategy)
            all_results.extend(strategy_results)
            
            # –ù–∞—Ö–æ–¥–∏–º –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è —ç—Ç–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
            if strategy_results:
                best_for_strategy = max(strategy_results, key=lambda x: x['MRR'])
                print(f"üèÜ –õ—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è {strategy}: MRR = {best_for_strategy['MRR']:.3f}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        self.save_results(all_results)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        self.analyze_results(all_results)
        
        return all_results
    
    def save_results(self, results: List[Dict]):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""
        os.makedirs("./data/optimization_results", exist_ok=True)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ CSV
        df_results = pd.DataFrame(results)
        df_results.to_csv("./data/optimization_results/all_experiments.csv", 
                         index=False, encoding='utf-8')
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ JSON –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        with open("./data/optimization_results/detailed_results.json", "w", encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ ./data/optimization_results/")
    
    def analyze_results(self, results: List[Dict]):
        """–ê–Ω–∞–ª–∏–∑ –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        if not results:
            print("‚ùå –ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
            return
        
        df = pd.DataFrame(results)
        
        # –ù–∞—Ö–æ–¥–∏–º –∞–±—Å–æ–ª—é—Ç–Ω–æ –ª—É—á—à—É—é –∫–æ–º–±–∏–Ω–∞—Ü–∏—é
        best_overall = df.loc[df['MRR'].idxmax()]
        
        print(f"\nüéØ –ê–ë–°–û–õ–Æ–¢–ù–û –õ–£–ß–®–ê–Ø –ö–û–ú–ë–ò–ù–ê–¶–ò–Ø:")
        print(f"–°—Ç—Ä–∞—Ç–µ–≥–∏—è: {best_overall['strategy']}")
        print(f"–ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {best_overall['params']}")
        print(f"MRR: {best_overall['MRR']:.3f}")
        print(f"Hits@1: {best_overall['Hits@1']:.3f}")
        print(f"Chunks: {best_overall['total_chunks']}")
        print(f"–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞: {best_overall['avg_chunk_words']:.1f} —Å–ª–æ–≤")
        
        # –ê–Ω–∞–ª–∏–∑ –ø–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º
        print(f"\nüìä –°–†–ê–í–ù–ï–ù–ò–ï –°–¢–†–ê–¢–ï–ì–ò–ô (–ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã):")
        best_by_strategy = df.loc[df.groupby('strategy')['MRR'].idxmax()]
        
        for _, row in best_by_strategy.iterrows():
            print(f"{row['strategy']:12} | MRR: {row['MRR']:.3f} | "
                  f"Hits@1: {row['Hits@1']:.3f} | Chunks: {row['total_chunks']:4} | "
                  f"Words: {row['avg_chunk_words']:5.1f}")
        
        # –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç —Ä–∞–∑–º–µ—Ä–∞ chunks
        print(f"\nüìà –ó–ê–í–ò–°–ò–ú–û–°–¢–¨ –ö–ê–ß–ï–°–¢–í–ê –û–¢ –†–ê–ó–ú–ï–†–ê CHUNKS:")
        df_sorted = df.sort_values('avg_chunk_words')
        for _, row in df_sorted.iterrows():
            if row['avg_chunk_words'] > 0:
                print(f"{row['avg_chunk_words']:5.1f} —Å–ª–æ–≤ | MRR: {row['MRR']:.3f} | "
                      f"{row['strategy']} {row['params']}")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        self.generate_recommendations(df)
    
    def generate_recommendations(self, df: pd.DataFrame):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        print(f"\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
        
        # –õ—É—á—à–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è
        best = df.loc[df['MRR'].idxmax()]
        print(f"1. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏—é: {best['strategy']}")
        print(f"2. –° –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏: {best['params']}")
        
        # –ê–Ω–∞–ª–∏–∑ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
        efficient = df[df['total_chunks'] < 500].nlargest(3, 'MRR')
        if len(efficient) > 0:
            print(f"3. –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã (–º–∞–ª–æ chunks, –≤—ã—Å–æ–∫–∏–π MRR):")
            for _, row in efficient.iterrows():
                print(f"   - {row['strategy']}: MRR {row['MRR']:.3f}, {row['total_chunks']} chunks")
        
        # –û–±—â–∏–µ –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–∏
        avg_words_best = df.nlargest(5, 'MRR')['avg_chunk_words'].mean()
        print(f"4. –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä chunks: {avg_words_best:.1f} —Å–ª–æ–≤ –≤ —Å—Ä–µ–¥–Ω–µ–º")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üöÄ AUTOMATIC CHUNK OPTIMIZATION SYSTEM")
    print("=" * 60)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    print("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    docs = pd.read_csv("./data/dataset/texts.csv")
    questions = pd.read_csv("./data/dataset/questions.csv")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    print("–ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
    raw_texts = []
    for index, row in docs.iterrows():
        try:
            file_path = f"./data/dataset/texts/{row['page_id']}.txt"
            with open(file_path, "r", encoding='utf-8') as f:
                raw_texts.append(f.read())
        except FileNotFoundError:
            raw_texts.append("")
    
    docs["text"] = raw_texts
    
    print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ: {len(docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, {len(questions)} –≤–æ–ø—Ä–æ—Å–æ–≤")
    
    # –°–æ–∑–¥–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
    optimizer = ChunkOptimizer(docs, questions)
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é
    start_time = time.time()
    all_results = optimizer.run_comprehensive_optimization()
    end_time = time.time()
    
    print(f"\n‚è±Ô∏è  –û–±—â–µ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {(end_time - start_time)/60:.1f} –º–∏–Ω—É—Ç")
    print(f"üìä –ü—Ä–æ–≤–µ–¥–µ–Ω–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤: {len(all_results)}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
    if all_results:
        best_result = max(all_results, key=lambda x: x['MRR'])
        report = {
            "best_strategy": best_result['strategy'],
            "best_params": best_result['params'],
            "best_mrr": best_result['MRR'],
            "best_hits1": best_result['Hits@1'],
            "total_experiments": len(all_results),
            "optimization_time_minutes": (end_time - start_time)/60
        }
        
        with open("./data/optimization_results/final_report.json", "w", encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"\n‚úÖ –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø –ó–ê–í–ï–†–®–ï–ù–ê!")
        print(f"üéØ –†–ï–ö–û–ú–ï–ù–î–£–ï–ú–ê–Ø –ù–ê–°–¢–†–û–ô–ö–ê:")
        print(f"   –°—Ç—Ä–∞—Ç–µ–≥–∏—è: {best_result['strategy']}")
        print(f"   –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {best_result['params']}")
        print(f"   –û–∂–∏–¥–∞–µ–º—ã–π MRR: {best_result['MRR']:.3f}")

if __name__ == "__main__":
    main()